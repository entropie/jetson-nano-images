version: "3.8"

services:
  llava_phi_3_mini:
    build:
      context: .
      dockerfile: Dockerfile
    image: llava-phi-3-mini-gguf-server:latest
    container_name: llava_phi_3_mini
    ports:
      - "8092:8080"
    volumes:
      - /data:/data
    environment:
      HF_TOKEN: ${HF_TOKEN:?set HF_TOKEN in your .env}
      MODEL_REPO: xtuner/llava-phi-3-mini-gguf
      MODEL_FILE: llava-phi-3-mini-int4.gguf
      MMPROJ_FILE: llava-phi-3-mini-mmproj-f16.gguf
      HOST: 0.0.0.0
      PORT: "8080"
      CTX: "2048"
      NGL: "999"
      THREADS: "6"
      GGML_CUDA_FORCE_CUBLAS: "1"
      GGML_CUDA_DISABLE_GRAPHS: "1"
    devices:
      - "nvidia.com/gpu=all"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/v1/models >/dev/null || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s
